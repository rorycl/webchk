go to each page in a website
	get the links on that page
	report http status if not 200
	report if any search words are found


use 8 concurrent workers

each gets a url
	receives on a chan of url
	reports on a chan of urlResults
		thisURL
		[]link
		status
		[]lineno/match
		err
	waits for done

	main 
		starts workers
		starts sending start url on url chan
		receives urlResults
			sends links out on url chan if it meets criteria
		
		for each put on url chan wg.Add()
		for each receipt wg.Done()
		done channel close waits for wg.Done()
	
	



var recurseDepth INT

getURL(link string, phrases []string, thisDepth INT) {

	# gets html page at link or prints http error code
	# extracts links and recursively calls them
      but only follows links in this domain
	  and only recurses to recursedepth (default 5)
	# looks for each phrase and, if it finds them prints the page and line number

}

how to extract a links from a page

how to filter to check domain name from a link

	func Parse(rawURL string) (*URL, error)	
	net/url.URL.Host

how to only follow html links (or ignore responses that don't have html)



how to read body


	
	
